{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7-Gs-TftCIZ"
   },
   "source": [
    "# Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipcCco5-sdTH",
    "outputId": "e8e9d373-31df-4fc6-ecc5-a4748fc4ff58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Of Pages!!!\n",
      "Total number of publications: 285\n",
      "Total number of distinct authors: 51\n",
      "Maximum publications per author: 47\n",
      "Results saved to publication_titles_and_hyperlinks.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Function to check the robots.txt file and get crawl delay (if available)\n",
    "def get_crawl_delay(url):\n",
    "    robots_url = urljoin(url, \"/robots.txt\")\n",
    "    response = requests.get(robots_url)\n",
    "    if response.status_code == 200:\n",
    "        robots_content = response.text\n",
    "        for line in robots_content.split('\\n'):\n",
    "            if line.startswith(\"Crawl-delay:\"):\n",
    "                delay = float(line.split(\":\")[1].strip())\n",
    "                return delay\n",
    "    return None\n",
    "\n",
    "# Function to scrape publication details from a given URL with delay\n",
    "def scrape_publications(url, crawl_delay):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    publications = []\n",
    "\n",
    "    for publication in soup.select('li.list-result-item'):\n",
    "        title_element = publication.select_one('h3.title > a')\n",
    "        title = title_element.get_text(strip=True) if title_element else \"Title Not Found\"\n",
    "        publication_url = title_element['href'] if title_element else \"Publication URL Not Found\"\n",
    "\n",
    "        author_element = publication.select_one('a.link.person')\n",
    "        author = author_element.get_text(strip=True) if author_element else \"Author Not Found\"\n",
    "        author_url = author_element['href'] if author_element else \"Author URL Not Found\"\n",
    "\n",
    "        publication_year_element = publication.select_one('span.date')\n",
    "        publication_year = publication_year_element.get_text(strip=True) if publication_year_element else \"Publication Year Not Found\"\n",
    "\n",
    "        publications.append((title, author, publication_year, publication_url, author_url))\n",
    "\n",
    "    # Introduce a delay to be polite to the website\n",
    "    if crawl_delay:\n",
    "        time.sleep(crawl_delay)\n",
    "\n",
    "    return publications\n",
    "\n",
    "# Function to save the results to a CSV file\n",
    "def save_to_csv(publications, csv_file):\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Title', 'Author', 'Publication_Year', 'Publication_URL', 'Author_URL'])\n",
    "        for title, author, publication_year, publication_url, author_url in publications:\n",
    "            writer.writerow([title, author, publication_year, publication_url, author_url])\n",
    "\n",
    "# Main function to initiate crawling and saving data to CSV\n",
    "def main():\n",
    "    base_url = \"https://pureportal.coventry.ac.uk\"\n",
    "    start_url = urljoin(base_url, \"/en/organisations/centre-global-learning/publications/\")\n",
    "    csv_file = 'publication_titles_and_hyperlinks.csv'\n",
    "\n",
    "    # Check crawl delay from robots.txt\n",
    "    crawl_delay = get_crawl_delay(base_url)\n",
    "    if crawl_delay:\n",
    "        print(\"Crawl delay:\", crawl_delay, \"seconds\")\n",
    "\n",
    "    # Initialize an empty list to store all publications\n",
    "    all_publications = []\n",
    "\n",
    "    # Initialize variables for pagination\n",
    "    current_page = 1\n",
    "    count = 0\n",
    "\n",
    "    # Crawl the pages and extract data with delay\n",
    "    while True:\n",
    "        result_temp = scrape_publications(start_url + f\"?page={current_page-1}\", crawl_delay)\n",
    "\n",
    "        # Check if any results were found on the current page\n",
    "        if not result_temp:\n",
    "            # If there are no results, stop crawling\n",
    "            print('End Of Pages!!!')\n",
    "            break\n",
    "\n",
    "        if result_temp[0][0] == \"Title Not Found\" and result_temp[0][1] == \"Author Not Found\" and result_temp[0][2] == \"Publication Year Not Found\":\n",
    "            # If there are no results, stop crawling\n",
    "            # print('End Of Pages!!!')\n",
    "            break\n",
    "        else:\n",
    "            all_publications.extend(result_temp)\n",
    "            # print(f\"Scraped {len(result_temp)} publications from {start_url}?page={current_page}\")\n",
    "            count += len(result_temp)\n",
    "            current_page += 1\n",
    "\n",
    "    print(\"Total number of publications:\", count)\n",
    "\n",
    "    # Find the total number of distinct authors and maximum publications per author\n",
    "    authors = {}  # Dictionary to store author names and their publication counts\n",
    "    max_publications = 0\n",
    "\n",
    "    for title, author, publication_year, publication_url, author_url in all_publications:\n",
    "        if author not in authors:\n",
    "            authors[author] = 1\n",
    "        else:\n",
    "            authors[author] += 1\n",
    "\n",
    "        # Updating most publications per author\n",
    "        if authors[author] > max_publications:\n",
    "            max_publications = authors[author]\n",
    "\n",
    "    print(\"Total number of distinct authors:\", len(authors))\n",
    "    print(\"Maximum publications per author:\", max_publications)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    save_to_csv(all_publications, csv_file)\n",
    "    print(\"Results saved to\", csv_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o8Mp60jtGHM"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcnSUS4Xsu5v",
    "outputId": "64ab2f2c-4309-43d7-e07a-0fb63f965a82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to CSV file: publication_data_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to apply pre-processing tasks to text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-alphanumeric characters and split the text into words\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "\n",
    "\n",
    "    # Join the processed words back into a string\n",
    "    processed_text = ' '.join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "url = \"https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/\"\n",
    "page = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "all_titles = soup.find_all(\"h3\", class_=\"title\")\n",
    "all_author = soup.find_all(\"a\", class_=\"link person\")\n",
    "publication_years = soup.find_all(\"span\", class_=\"date\")\n",
    "author_links = [author.get('href') for author in all_author]\n",
    "title_links = [title.a.get('href') for title in all_titles]\n",
    "\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "\n",
    "for title, author, pub_year, author_link, link in zip(all_titles, all_author, publication_years, all_author, title_links):\n",
    "    # Extract the required information\n",
    "    title_text = title.span.text.strip()\n",
    "    author_text = author.string\n",
    "    pub_year_text = pub_year.text\n",
    "    author_link_text = author_link.get('href')\n",
    "    publication_link = title.a.get('href')\n",
    "\n",
    "    # Clean and preprocess the title and author text\n",
    "    cleaned_title = re.sub(r'[^\\w\\s]', '', title_text)  # Remove special characters\n",
    "    cleaned_author = re.sub(r'[^\\w\\s]', '', author_text)  # Remove special characters\n",
    "    processed_title = preprocess_text(cleaned_title)\n",
    "    processed_author = preprocess_text(cleaned_author)\n",
    "\n",
    "    # Append the data to the list with additional metadata (if available)\n",
    "    data.append([title_text, author_text, pub_year_text, author_link_text, publication_link,\n",
    "                 processed_title, processed_author])\n",
    "\n",
    "# Define the CSV file name\n",
    "csv_file = \"publication_data_preprocessed.csv\"\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Title\", \"Author\", \"Publication Years\", \"Authors Profile\", \"Publication Link\",\n",
    "                     \"Processed Title\", \"Processed Author\"])\n",
    "    # Write the data rows\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"Preprocessed data saved to CSV file:\", csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zf5HL853tK8l"
   },
   "source": [
    "# Inverted Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0QdOmQbkt0Ji",
    "outputId": "d090267e-a775-4560-9acb-e2021ae91636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index:\n",
      "revisit: [1]\n",
      "role: [1, 18]\n",
      "gender: [1, 47]\n",
      "moderating: [1]\n",
      "effect: [1]\n",
      "emotional: [1, 40]\n",
      "intelligence: [1, 40]\n",
      "leadership: [1, 20]\n",
      "effectiveness: [1]\n",
      "study: [1, 39, 43]\n",
      "Indexed data saved to CSV file: manual_inverted_index.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to load preprocessed data from CSV file\n",
    "def load_preprocessed_data(csv_file):\n",
    "    data = []\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            title, processed_title = row\n",
    "            data.append((title, processed_title))\n",
    "    return data\n",
    "\n",
    "# Function to build the inverted index\n",
    "def build_inverted_index(docs):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, (_, processed_title) in enumerate(docs, start=1):\n",
    "        terms = processed_title.split()\n",
    "        for term in terms:\n",
    "            index[term].append(doc_id)\n",
    "    return index\n",
    "\n",
    "# Load preprocessed data from CSV file\n",
    "preprocessed_csv_file = \"publication_data_preprocessed.csv\"\n",
    "docs = load_preprocessed_data(preprocessed_csv_file)\n",
    "\n",
    "# Build the inverted index\n",
    "index = build_inverted_index(docs)\n",
    "\n",
    "# Define the CSV file name for the indexed data (inverted index)\n",
    "indexed_csv_file = \"manual_inverted_index.csv\"\n",
    "\n",
    "# Write the indexed data (inverted index) to the CSV file\n",
    "with open(indexed_csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Term\", \"Document IDs\"])\n",
    "    # Write the data rows\n",
    "    writer.writerows(index.items())\n",
    "\n",
    "# Print some part of the constructed inverted index\n",
    "print(\"Inverted Index:\")\n",
    "for term, doc_ids in list(index.items())[:10]:\n",
    "    print(f\"{term}: {doc_ids}\")\n",
    "\n",
    "print(\"Indexed data saved to CSV file:\", indexed_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50F9QieUuJML"
   },
   "source": [
    "# Incremental Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DX0oEiO_uM89",
    "outputId": "cc0054bb-80bb-4eb5-87f1-cfe72bb0ccc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index:\n",
      "revisit: [1]\n",
      "role: [1, 18]\n",
      "gender: [1, 47]\n",
      "moderating: [1]\n",
      "effect: [1]\n",
      "emotional: [1, 40]\n",
      "intelligence: [1, 40]\n",
      "leadership: [1, 20]\n",
      "effectiveness: [1]\n",
      "study: [1, 39, 43]\n",
      "Indexed data saved to CSV file: manual_inverted_index.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to load preprocessed data from CSV file\n",
    "def load_preprocessed_data(csv_file):\n",
    "    data = []\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            title, processed_title = row\n",
    "            data.append((title, processed_title))\n",
    "    return data\n",
    "\n",
    "# Function to build the inverted index\n",
    "def build_inverted_index(docs):\n",
    "    index = defaultdict(list)\n",
    "    for doc_id, (_, processed_title) in enumerate(docs, start=1):\n",
    "        terms = processed_title.split()\n",
    "        for term in terms:\n",
    "            index[term].append(doc_id)\n",
    "    return index\n",
    "\n",
    "# Load preprocessed data from CSV file\n",
    "preprocessed_csv_file = \"publication_data_preprocessed.csv\"\n",
    "docs = load_preprocessed_data(preprocessed_csv_file)\n",
    "\n",
    "# Define the CSV file name for the indexed data (inverted index)\n",
    "indexed_csv_file = \"manual_inverted_index.csv\"\n",
    "\n",
    "try:\n",
    "    # Load the existing index from CSV file\n",
    "    with open(indexed_csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        existing_index = defaultdict(list)\n",
    "        for row in reader:\n",
    "            term, doc_ids = row\n",
    "            existing_index[term] = [int(doc_id) for doc_id in doc_ids.strip('[]').split(',') if doc_id.strip()]\n",
    "except FileNotFoundError:\n",
    "    # If the index file is not found, initialize an empty index\n",
    "    existing_index = defaultdict(list)\n",
    "\n",
    "# Update the index with new data from the crawler component\n",
    "new_data = [\n",
    "    (\"New Publication Title 1\", \"new publication title 1 processed\"),\n",
    "    (\"New Publication Title 2\", \"new publication title 2 processed\"),\n",
    "    # Add new data in the same format as the preprocessed data (title, processed_title)\n",
    "    # Replace the above dummy data with the actual data received from the crawler\n",
    "]\n",
    "\n",
    "for doc_id, (_, processed_title) in enumerate(new_data, start=len(docs) + 1):\n",
    "    terms = processed_title.split()\n",
    "    for term in terms:\n",
    "        existing_index[term].append(doc_id)\n",
    "\n",
    "# Save the updated index back to the CSV file\n",
    "with open(indexed_csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Term\", \"Document IDs\"])\n",
    "    # Write the data rows\n",
    "    for term, doc_ids in existing_index.items():\n",
    "        writer.writerow([term, ','.join(map(str, doc_ids))])\n",
    "\n",
    "# Print some part of the constructed inverted index\n",
    "print(\"Inverted Index:\")\n",
    "for term, doc_ids in list(existing_index.items())[:10]:\n",
    "    print(f\"{term}: {doc_ids}\")\n",
    "\n",
    "print(\"Indexed data saved to CSV file:\", indexed_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpIu0jP1yBPd"
   },
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "id": "qiSlhPUsyBin",
    "outputId": "0d5875c1-c7ad-4bbf-e338-2e76c5c593e6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMvk5FEhyOQW",
    "outputId": "0edea239-e52b-44fa-be8a-c04c08b7136a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (or 'exit' to quit): nigeian\n",
      "No matching publications found.\n",
      "Enter your query (or 'exit' to quit): sensitivity\n",
      "No matching publications found.\n",
      "Enter your query (or 'exit' to quit): becoming nigerian\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: Becoming Nigerian\n",
      "Relevance Score: 25.665085570920002\n",
      "--------------------------------------------\n",
      "Publication Title: What does it Mean to be Educated in Nigerian Student Experience?\n",
      "Relevance Score: 10.361161575920939\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): mechanism\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: 'Nordic added value’: a floating signifier and a mechanism for Nordic higher education regionalism\n",
      "Relevance Score: 15.303923994999064\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): cultural \n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: Interactive Media for Cultural Heritage book: \"From Headsets to Mindsets: A Taxonomy for Human-centred Extended Reality Experimentations for Cultural Heritage.\"\n",
      "Relevance Score: 41.444646303683754\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): multi-level\n",
      "No matching publications found.\n",
      "Enter your query (or 'exit' to quit): multi\n",
      "No matching publications found.\n",
      "Enter your query (or 'exit' to quit): understanding\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: Understanding the Academic Achievement of the First- and Second-Generation Immigrant Students: A Multi-level Analysis of PISA 2018 Data\n",
      "Relevance Score: 15.303923994999064\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): 75 years\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: 75 Years of Scouting/Guiding Activities in Pakistan - Celebrating the Shared British Heritage of Active Citizenship\n",
      "Relevance Score: 20.605822105477465\n",
      "--------------------------------------------\n",
      "Publication Title: Indonesian 2013 Curriculum Ten Years On: Impact on Mathematics Teaching\n",
      "Relevance Score: 5.301898110478399\n",
      "--------------------------------------------\n",
      "Publication Title: The impact of Covid-19 on mathematical entry competencies: 1 year on\n",
      "Relevance Score: 5.301898110478399\n",
      "--------------------------------------------\n",
      "Publication Title: Evaluation of the ThinkHigher Uni Connect Programme Coventry and Warwickshire, in Year 10 students\n",
      "Relevance Score: 5.301898110478399\n",
      "--------------------------------------------\n",
      "Publication Title: Evaluation of the ThinkHigher Uni Connect Programme Coventry and Warwickshire, in Year 12 students\n",
      "Relevance Score: 5.301898110478399\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to apply pre-processing tasks to text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text and remove non-alphanumeric characters\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the processed words back into a string\n",
    "    processed_text = ' '.join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Function to load the indexed data from CSV file\n",
    "def load_indexed_data(csv_file):\n",
    "    index = defaultdict(list)\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            term, doc_ids = row\n",
    "            index[term] = [int(doc_id) for doc_id in doc_ids.strip('[]').split(',') if doc_id.strip()]\n",
    "    return index\n",
    "\n",
    "# Function to perform ranked retrieval using vector space model\n",
    "def ranked_retrieval(query, index, docs, top_n=10):\n",
    "    query = preprocess_text(query)\n",
    "    query_terms = query.split()\n",
    "\n",
    "    # Calculate the document frequency for each query term\n",
    "    doc_freq = {term: len(index[term]) for term in query_terms if term in index}\n",
    "\n",
    "    # Calculate the inverse document frequency for each query term\n",
    "    N = len(docs)  # Total number of documents\n",
    "    idf = {term: math.log(N / df) for term, df in doc_freq.items()}\n",
    "\n",
    "    # Calculate the term frequency in the query\n",
    "    tf_query = {term: query_terms.count(term) for term in query_terms}\n",
    "\n",
    "    # Calculate the vector space representation of the query\n",
    "    query_vector = {term: tf_query[term] * idf[term] for term in query_terms if term in idf}\n",
    "\n",
    "    # Calculate the relevance score (rank) for each document\n",
    "    relevance_scores = defaultdict(float)\n",
    "    for term in query_terms:\n",
    "        if term in index:\n",
    "            for doc_id in index[term]:\n",
    "                tf_doc = docs[doc_id - 1][1].split().count(term)  # Term frequency in the document\n",
    "                tfidf = tf_doc * idf[term]  # Term frequency-inverse document frequency\n",
    "                relevance_scores[doc_id] += tfidf * query_vector[term]\n",
    "\n",
    "    # Sort the documents based on relevance scores in descending order\n",
    "    ranked_docs = sorted(relevance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N relevant documents\n",
    "    top_docs = ranked_docs[:top_n]\n",
    "\n",
    "    return top_docs\n",
    "\n",
    "# Load the indexed data (inverted index) from the CSV file\n",
    "indexed_csv_file = \"manual_inverted_index.csv\"\n",
    "index = load_indexed_data(indexed_csv_file)\n",
    "\n",
    "# Load preprocessed data from CSV file\n",
    "preprocessed_csv_file = \"publication_data_preprocessed.csv\"\n",
    "with open(preprocessed_csv_file, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)  # Skip the header row\n",
    "    docs = [(title, processed_title) for title, processed_title in reader]\n",
    "\n",
    "# Command-line interface\n",
    "while True:\n",
    "    user_query = input(\"Enter your query (or 'exit' to quit): \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Perform ranked retrieval and display the top 5 publications\n",
    "    top_publications = ranked_retrieval(user_query, index, docs, top_n=5)\n",
    "\n",
    "    if not top_publications:\n",
    "        print(\"No matching publications found.\")\n",
    "    else:\n",
    "        print(\"Top 5 Relevant Publications:\")\n",
    "        for doc_id, relevance_score in top_publications:\n",
    "            title = docs[doc_id - 1][0]\n",
    "            print(f\"Publication Title: {title}\")\n",
    "            print(f\"Relevance Score: {relevance_score}\")\n",
    "            print(\"--------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnJEY5Fw2PBW",
    "outputId": "894b2b19-0970-4c10-fe08-a78b4229d05b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (or 'exit' to quit): zoltán,\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: Obituary for Zoltán Dörnyei (1960–2022): a bibliometric mapping of his publications\n",
      "Relevance Score: 15.303923994999064\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): Zoltan,\n",
      "No matching publications found.\n",
      "Enter your query (or 'exit' to quit): mapping,\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: Obituary for Zoltán Dörnyei (1960–2022): a bibliometric mapping of his publications\n",
      "Relevance Score: 10.361161575920939\n",
      "--------------------------------------------\n",
      "Publication Title: A bibliometric mapping of shadow education research: achievements, limitations, and the future\n",
      "Relevance Score: 10.361161575920939\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): mapping:\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: Obituary for Zoltán Dörnyei (1960–2022): a bibliometric mapping of his publications\n",
      "Relevance Score: 10.361161575920939\n",
      "--------------------------------------------\n",
      "Publication Title: A bibliometric mapping of shadow education research: achievements, limitations, and the future\n",
      "Relevance Score: 10.361161575920939\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): edu\n",
      "No matching publications found.\n",
      "Enter your query (or 'exit' to quit): education\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: Coming out of the shadows: Investing in English private tutoring at a transition point in Kazakhstan’s education system during the global pandemic\n",
      "Relevance Score: 2.5902903939802346\n",
      "--------------------------------------------\n",
      "Publication Title: 'Nordic added value’: a floating signifier and a mechanism for Nordic higher education regionalism\n",
      "Relevance Score: 2.5902903939802346\n",
      "--------------------------------------------\n",
      "Publication Title: Throwing light on fee-charging tutoring during the global pandemic in Kazakhstan: implications for the future of higher education\n",
      "Relevance Score: 2.5902903939802346\n",
      "--------------------------------------------\n",
      "Publication Title: Transformational school leadership: a systematic review of research in a centralized education system\n",
      "Relevance Score: 2.5902903939802346\n",
      "--------------------------------------------\n",
      "Publication Title: A bibliometric mapping of shadow education research: achievements, limitations, and the future\n",
      "Relevance Score: 2.5902903939802346\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): 'Nordic added value’\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: 'Nordic added value’: a floating signifier and a mechanism for Nordic higher education regionalism\n",
      "Relevance Score: 45.91177198499719\n",
      "--------------------------------------------\n",
      "Enter your query (or 'exit' to quit): Nordic, added ,value.\n",
      "Top 5 Relevant Publications:\n",
      "Publication Title: 'Nordic added value’: a floating signifier and a mechanism for Nordic higher education regionalism\n",
      "Relevance Score: 45.91177198499719\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to apply pre-processing tasks to text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text and remove non-alphanumeric characters\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the processed words back into a string\n",
    "    processed_text = ' '.join(words)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Function to load the indexed data from CSV file\n",
    "def load_indexed_data(csv_file):\n",
    "    index = defaultdict(list)\n",
    "    with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            term, doc_ids = row\n",
    "            index[term] = [int(doc_id) for doc_id in doc_ids.strip('[]').split(',') if doc_id.strip()]\n",
    "    return index\n",
    "\n",
    "# Function to perform ranked retrieval using vector space model\n",
    "def ranked_retrieval(query, index, docs, top_n=10):\n",
    "    query = preprocess_text(query)\n",
    "    query_terms = query.split()\n",
    "\n",
    "    # Calculate the document frequency for each query term\n",
    "    doc_freq = {term: len(index[term]) for term in query_terms if term in index}\n",
    "\n",
    "    # Calculate the inverse document frequency for each query term\n",
    "    N = len(docs)  # Total number of documents\n",
    "    idf = {term: math.log(N / df) for term, df in doc_freq.items()}\n",
    "\n",
    "    # Calculate the term frequency in the query\n",
    "    tf_query = {term: query_terms.count(term) for term in query_terms}\n",
    "\n",
    "    # Calculate the vector space representation of the query\n",
    "    query_vector = {term: tf_query[term] * idf[term] for term in query_terms if term in idf}\n",
    "\n",
    "    # Calculate the relevance score (rank) for each document\n",
    "    relevance_scores = defaultdict(float)\n",
    "    for term in query_terms:\n",
    "        if term in index:\n",
    "            for doc_id in index[term]:\n",
    "                tf_doc = docs[doc_id - 1][1].split().count(term)  # Term frequency in the document\n",
    "                tfidf = tf_doc * idf[term]  # Term frequency-inverse document frequency\n",
    "                relevance_scores[doc_id] += tfidf * query_vector[term]\n",
    "\n",
    "    # Sort the documents based on relevance scores in descending order\n",
    "    ranked_docs = sorted(relevance_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top N relevant documents\n",
    "    top_docs = ranked_docs[:top_n]\n",
    "\n",
    "    return top_docs\n",
    "\n",
    "# Load the indexed data (inverted index) from the CSV file\n",
    "indexed_csv_file = \"manual_inverted_index.csv\"\n",
    "index = load_indexed_data(indexed_csv_file)\n",
    "\n",
    "# Load preprocessed data from CSV file\n",
    "preprocessed_csv_file = \"publication_data_preprocessed.csv\"\n",
    "with open(preprocessed_csv_file, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)  # Skip the header row\n",
    "    docs = [(title, processed_title) for title, processed_title in reader]\n",
    "\n",
    "# Command-line interface\n",
    "while True:\n",
    "    user_query = input(\"Enter your query (or 'exit' to quit): \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Perform ranked retrieval and display the top 5 publications\n",
    "    top_publications = ranked_retrieval(user_query, index, docs, top_n=5)\n",
    "\n",
    "    if not top_publications:\n",
    "        print(\"No matching publications found.\")\n",
    "    else:\n",
    "        print(\"Top 5 Relevant Publications:\")\n",
    "        for doc_id, relevance_score in top_publications:\n",
    "            title = docs[doc_id - 1][0]\n",
    "            print(f\"Publication Title: {title}\")\n",
    "            print(f\"Relevance Score: {relevance_score}\")\n",
    "            print(\"--------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
